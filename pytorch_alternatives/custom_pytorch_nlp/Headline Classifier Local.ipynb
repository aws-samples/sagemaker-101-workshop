{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NLP: News Headline Classifier (Local Version)\n",
    "\n",
    "_**Train a PyTorch-based model to classify news headlines between four domains**_\n",
    "\n",
    "This notebook works well with the `Python 3 (PyTorch 1.4 Python 3.6 CPU Optimized)` kernel on SageMaker Studio\n",
    "\n",
    "---\n",
    "\n",
    "In this version, the model is trained and evaluated here on the notebook instance itself. We'll show in the follow-on notebook how to take advantage of Amazon SageMaker to separate these infrastructure needs.\n",
    "\n",
    "Note that you can safely ignore the WARNING about the pip version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First install some libraries which might not be available across all kernels (e.g. in Studio):\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.5  # Depending on your PyTorch version https://pypi.org/project/torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download News Aggregator Dataset\n",
    "\n",
    "We will download **FastAi AG News** dataset from the https://registry.opendata.aws/fast-ai-nlp/ public repository. This dataset contains a table of news headlines and their corresponding classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import util.preprocessing\n",
    "\n",
    "util.preprocessing.download_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the dataset\n",
    "\n",
    "We will load the ag_news_csv/train.csv file to a Pandas dataframe for our data processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"CATEGORY\", \"TITLE\", \"CONTENT\"]\n",
    "# we use the train.csv only\n",
    "df = pd.read_csv(\"data/ag_news_csv/train.csv\", names=column_names, header=None, delimiter=\",\")\n",
    "# shuffle the DataFrame rows\n",
    "df = df.sample(frac = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll **only use**:\n",
    "\n",
    "- The **title** (Headline) of the news story, as our input\n",
    "- The **category**, as our target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has **four article categories:**\n",
    "\n",
    "- World (1)\n",
    "- Sports (2)\n",
    "- Business (3)\n",
    "- Sci/Tech (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Pre-Processing\n",
    "\n",
    "We'll do some basic processing of the text data to convert it into numerical form that the algorithm will be able to consume to create a model.\n",
    "\n",
    "We will do typical pre processing for NLP workloads such as: dummy encoding the labels, tokenizing the documents and set fixed sequence lengths for input feature dimension, padding documents to have fixed length input vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Encode the Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y, labels = util.preprocessing.dummy_encode_labels(df, \"CATEGORY\")\n",
    "print(labels)\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Set Fixed Sequence Lengths\n",
    "\n",
    "We want to describe our inputs at the more meaningful word level (rather than individual characters), and ensure a fixed length of the input feature dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7bcf422f-0e75-4d49-b3b1-12553fcaf4ff",
    "_uuid": "46b7fc9aef5a519f96a295e980ba15deee781e97"
   },
   "outputs": [],
   "source": [
    "processed_docs, tokenizer = util.preprocessing.tokenize_and_pad_docs(df, \"TITLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TITLE\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Word Embeddings\n",
    "\n",
    "To represent our words in numeric form, we'll use pre-trained vector representations for each word in the vocabulary: In this case we'll be using pre-built GloVe word embeddings.\n",
    "\n",
    "You could also explore training custom, domain-specific word embeddings using SageMaker's built-in [BlazingText algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html). See the official [blazingtext_word2vec_text8 sample](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/blazingtext_word2vec_text8) for an example notebook showing how.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_matrix = util.preprocessing.get_word_embeddings(tokenizer, \"data/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    file=\"./data/embeddings/docs-embedding-matrix\",\n",
    "    arr=embedding_matrix,\n",
    "    allow_pickle=False,\n",
    ")\n",
    "vocab_size=embedding_matrix.shape[0]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets\n",
    "\n",
    "Finally we need to divide our data into model training and evaluation sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    processed_docs,\n",
    "    encoded_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you always remember to save your datasets for traceability when experimenting locally? ;-)\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "np.save(\"./data/train/train_X.npy\", x_train)\n",
    "np.save(\"./data/train/train_Y.npy\", y_train)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "np.save(\"./data/test/test_X.npy\", x_test)\n",
    "np.save(\"./data/test/test_Y.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "num_classes=len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size = 400000, num_classes = 4):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 100)\n",
    "        self.conv1 = nn.Conv1d(100, 128, kernel_size=3)\n",
    "        self.max_pool1d = nn.MaxPool1d(5)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(896, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = self.flatten1(self.max_pool1d(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.binary_cross_entropy(output, target, reduction='mean').item()  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            target_index = target.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target_index).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\"val_loss: {:.4f}\".format(test_loss))\n",
    "    print(\"val_acc: {:.4f}\".format(correct/len(test_loader.dataset)))   \n",
    "\n",
    "def train(train_loader, test_loader, embedding_matrix, vocab_size = 400000, num_classes = 4, epochs = 12, learning_rate = 0.001):\n",
    "    ###### Setup model architecture ############\n",
    "    model = Net(vocab_size, num_classes)\n",
    "    model.embedding.weight = torch.nn.parameter.Parameter(torch.FloatTensor(embedding_matrix), False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "            data, target = X_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(\"train_loss: {:.6f}\".format(running_loss / (len(train_loader.dataset))))     \n",
    "        print(\"Evaluating model\")\n",
    "        test(model, test_loader, device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = torch.as_tensor(self.data[index]).long()\n",
    "        y = torch.as_tensor(self.labels[index])\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit (Train) and Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit the model here in the notebook:\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "model_dir = 'model/'\n",
    "trainloader = torch.utils.data.DataLoader(Dataset(x_train, y_train), batch_size=16,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(Dataset(x_test, y_test), batch_size=1,\n",
    "                                          shuffle=True )\n",
    "print(\"Training model\")\n",
    "model = train(trainloader, testloader, embedding_matrix, \n",
    "              vocab_size=vocab_size, num_classes=num_classes, epochs=epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (**JupyterLab / SageMaker Studio Only**) Installing IPyWidgets Extension\n",
    "\n",
    "This notebook uses a fun little interactive widget to query the classifier, which works out of the box in plain Jupyter on a SageMaker Notebook Instance - but in JupyterLab or SageMaker Studio requires an extension not installed by default.\n",
    "\n",
    "**If you're using JupyterLab on a SageMaker Notebook Instance**, you can install it via UI:\n",
    "\n",
    "- Select \"*Settings > Enable Extension Manager (experimental)*\" from the toolbar, and confirm to enable it\n",
    "- Click on the new jigsaw puzzle piece icon in the sidebar, to open the Extension Manager\n",
    "- Search for `@jupyter-widgets/jupyterlab-manager` (Scroll down - search results show up *below* the list of currently installed widgets!)\n",
    "- Click \"**Install**\" below the widget's description\n",
    "- Wait for the blue progress bar that appears by the search box\n",
    "- You should be prompted \"*A build is needed to include the latest changes*\" - select \"**Rebuild**\"\n",
    "- The progress bar should resume, and you should shortly see a \"Build Complete\" dialogue.\n",
    "- Select \"**Reload**\" to reload the webpage\n",
    "\n",
    "**If you're using SageMaker Studio**, you can install it via CLI:\n",
    "\n",
    "- Open a new launcher and select **System terminal** (and **not** *Image terminal*)\n",
    "- Change to the repository root folder (e.g. with `cd sagemaker-workshop-101`) and check with `pwd` (print working directory)\n",
    "- Run `./init-studio.sh` and refresh your browser page when the script is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model (Locally)\n",
    "\n",
    "Let's evaluate our model with some example headlines...\n",
    "\n",
    "If you struggle with the widget, you can always simply call the `classify()` function from Python. You can be creative with your headlines!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def classify(text):\n",
    "    \"\"\"Classify a headline and print the results\"\"\"\n",
    "    processed = tokenizer.preprocess(text)\n",
    "    padded = tokenizer.pad([processed])\n",
    "    final_text = []\n",
    "    for w in padded[0]:\n",
    "        final_text.append(tokenizer.vocab.stoi[w])\n",
    "    final_text = torch.tensor([final_text])\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    result = model(final_text)\n",
    "    print(result)\n",
    "    ix = np.argmax(result.detach())\n",
    "    print(f\"Predicted class: '{labels[ix]}' with confidence {result[0][ix]:.2%}\")\n",
    "\n",
    "interaction = widgets.interact_manual(\n",
    "    classify,\n",
    "    text=widgets.Text(\n",
    "        value=\"The markets were bullish after news of the merger\",\n",
    "        placeholder=\"Type a news headline...\",\n",
    "        description=\"Headline:\",\n",
    "        layout=widgets.Layout(width=\"99%\"),\n",
    "    )\n",
    ")\n",
    "interaction.widget.children[1].description = \"Classify!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for the classes:\n",
    "\n",
    "- World (1)\n",
    "- Sports (2)\n",
    "- Business (3)\n",
    "- Sci/Tech (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this notebook we pre-processed publicly downloadable data and trained a neural news headline classifier model: As a data scientist might normally do when working on a local machine.\n",
    "\n",
    "...But can we use the cloud more effectively to allocate high-performance resources; and easily deploy our trained models for use by other applications?\n",
    "\n",
    "Head on over to the next notebook, [Headline Classifier SageMaker.ipynb](Headline%20Classifier%20SageMaker.ipynb), where we'll show how the same model can be trained and then deployed on specific target infrastructure with Amazon SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.4 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/pytorch-1.4-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
