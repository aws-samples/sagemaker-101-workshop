{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NLP: News Headline Classifier (Local Version)\n",
    "\n",
    "_**Train a PyTorch-based model to classify news headlines between four domains**_\n",
    "\n",
    "This notebook works well with the `Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)` kernel on SageMaker Studio, or `conda_pytorch_p36` on classic SageMaker Notebook Instances.\n",
    "\n",
    "---\n",
    "\n",
    "In this version, the model is trained and evaluated here on the notebook instance itself. We'll show in the follow-on notebook how to take advantage of Amazon SageMaker to separate these infrastructure needs.\n",
    "\n",
    "Note that you can safely ignore the WARNING about the pip version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First install some libraries which might not be available across all kernels (e.g. in Studio):\n",
    "# torchtext version will depend on your PyTorch version - see https://pypi.org/project/torchtext/\n",
    "!pip install ipywidgets torchtext==0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download News Aggregator Dataset\n",
    "\n",
    "We will download **FastAi AG News** dataset from the https://registry.opendata.aws/fast-ai-nlp/ public repository. This dataset contains a table of news headlines and their corresponding classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import util.preprocessing\n",
    "\n",
    "util.preprocessing.download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the dataset\n",
    "\n",
    "We will load the ag_news_csv/train.csv file to a Pandas dataframe for our data processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"CATEGORY\", \"TITLE\", \"CONTENT\"]\n",
    "# we use the train.csv only\n",
    "df = pd.read_csv(\"data/ag_news_csv/train.csv\", names=column_names, header=None, delimiter=\",\")\n",
    "# shuffle the DataFrame rows\n",
    "df = df.sample(frac=1, random_state=1337)\n",
    "# make the category classes more readable\n",
    "mapping = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "df = df.replace({'CATEGORY': mapping})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll **only use**:\n",
    "\n",
    "- The **title** (Headline) of the news story, as our input\n",
    "- The **category**, as our target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has **four article categories** with equal weighting:\n",
    "\n",
    "- Business\n",
    "- Sci/Tech\n",
    "- Sports\n",
    "- World\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Pre-Processing\n",
    "\n",
    "We'll do some basic processing of the text data to convert it into numerical form that the algorithm will be able to consume to create a model.\n",
    "\n",
    "We will do typical pre processing for NLP workloads such as: dummy encoding the labels, tokenizing the documents and set fixed sequence lengths for input feature dimension, padding documents to have fixed length input vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Encode the Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y, labels = util.preprocessing.dummy_encode_labels(df, \"CATEGORY\")\n",
    "print(labels)\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, looking at the first record in our (shuffled) dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Set Fixed Sequence Lengths\n",
    "\n",
    "We want to describe our inputs at the more meaningful word level (rather than individual characters), and ensure a fixed length of the input feature dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7bcf422f-0e75-4d49-b3b1-12553fcaf4ff",
    "_uuid": "46b7fc9aef5a519f96a295e980ba15deee781e97"
   },
   "outputs": [],
   "source": [
    "processed_docs, tokenizer = util.preprocessing.tokenize_and_pad_docs(df, \"TITLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TITLE\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Word Embeddings\n",
    "\n",
    "To represent our words in numeric form, we'll use pre-trained vector representations for each word in the vocabulary: In this case we'll be using [pre-trained word embeddings from FastText](https://fasttext.cc/docs/en/crawl-vectors.html), which are also available for a broad range of languages other than English.\n",
    "\n",
    "You could also explore training custom, domain-specific word embeddings using SageMaker's built-in [BlazingText algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html). See the official [blazingtext_word2vec_text8 sample](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/blazingtext_word2vec_text8) for an example notebook showing how.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_matrix = util.preprocessing.get_word_embeddings(tokenizer, \"data/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    file=\"./data/embeddings/docs-embedding-matrix\",\n",
    "    arr=embedding_matrix,\n",
    "    allow_pickle=False,\n",
    ")\n",
    "vocab_size=embedding_matrix.shape[0]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets\n",
    "\n",
    "Finally we need to divide our data into model training and evaluation sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    processed_docs,\n",
    "    encoded_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you always remember to save your datasets for traceability when experimenting locally? ;-)\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "np.save(\"./data/train/train_X.npy\", x_train)\n",
    "np.save(\"./data/train/train_Y.npy\", y_train)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "np.save(\"./data/test/test_X.npy\", x_test)\n",
    "np.save(\"./data/test/test_Y.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "num_classes=len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=400000, emb_dim=300, num_classes=4):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 128, kernel_size=3)\n",
    "        self.max_pool1d = nn.MaxPool1d(5)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(896, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = self.flatten1(self.max_pool1d(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.binary_cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            target_index = target.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target_index).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over dataset samples\n",
    "    print(f\"val_loss: {test_loss:.4f}, val_acc: {correct/len(test_loader.dataset):.4f}\") \n",
    "\n",
    "def train(train_loader, test_loader, embedding_matrix, num_classes=4, epochs=12, learning_rate=0.001):\n",
    "    ###### Setup model architecture ############\n",
    "    model = Net(\n",
    "        vocab_size=embedding_matrix.shape[0],\n",
    "        emb_dim=embedding_matrix.shape[1],\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model.embedding.weight = torch.nn.parameter.Parameter(torch.FloatTensor(embedding_matrix), False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for batch_idx, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "            data, target = X_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        print(f\"epoch: {epoch}, train_loss: {running_loss / n_batches:.6f}\")  # (Avg over batches)\n",
    "        print(\"Evaluating model\")\n",
    "        test(model, test_loader, device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = torch.as_tensor(self.data[index]).long()\n",
    "        y = torch.as_tensor(self.labels[index])\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit (Train) and Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit the model here in the notebook:\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "model_dir = \"model/\"\n",
    "trainloader = torch.utils.data.DataLoader(Dataset(x_train, y_train), batch_size=16, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(Dataset(x_test, y_test), batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training model\")\n",
    "model = train(\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    embedding_matrix,\n",
    "    num_classes=num_classes,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model (Locally)\n",
    "\n",
    "Let's evaluate our model with some example headlines...\n",
    "\n",
    "If you struggle with the widget, you can always simply call the `classify()` function from Python. You can be creative with your headlines!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def classify(text):\n",
    "    \"\"\"Classify a headline and print the results\"\"\"\n",
    "    processed = tokenizer.preprocess(text)\n",
    "    padded = tokenizer.pad([processed])\n",
    "    final_text = []\n",
    "    for w in padded[0]:\n",
    "        final_text.append(tokenizer.vocab.stoi[w])\n",
    "    final_text = torch.tensor([final_text])\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = model(final_text)\n",
    "    print(result)\n",
    "    ix = np.argmax(result.detach())\n",
    "    print(f\"Predicted class: '{labels[ix]}' with confidence {result[0][ix]:.2%}\")\n",
    "\n",
    "\n",
    "# Either try out the interactive widget:\n",
    "interaction = widgets.interact_manual(\n",
    "   classify,\n",
    "   text=widgets.Text(\n",
    "       value=\"The markets were bullish after news of the merger\",\n",
    "       placeholder=\"Type a news headline...\",\n",
    "       description=\"Headline:\",\n",
    "       layout=widgets.Layout(width=\"99%\"),\n",
    "   )\n",
    ")\n",
    "interaction.widget.children[1].description = \"Classify!\"\n",
    "\n",
    "# Or just use the function to classify your own headline:\n",
    "#classify(\"Retailers are expanding after the recent economic growth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this notebook we pre-processed publicly downloadable data and trained a neural news headline classifier model: As a data scientist might normally do when working on a local machine.\n",
    "\n",
    "...But can we use the cloud more effectively to allocate high-performance resources; and easily deploy our trained models for use by other applications?\n",
    "\n",
    "Head on over to the next notebook, [Headline Classifier SageMaker.ipynb](Headline%20Classifier%20SageMaker.ipynb), where we'll show how the same model can be trained and then deployed on specific target infrastructure with Amazon SageMaker.\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
