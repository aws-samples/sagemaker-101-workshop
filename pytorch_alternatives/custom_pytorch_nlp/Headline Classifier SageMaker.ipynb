{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NLP: News Headline Classifier (SageMaker Version)\n",
    "\n",
    "_**Train a PyTorch-based model to classify news headlines between four domains**_\n",
    "\n",
    "This notebook works well with the `Python 3 (PyTorch 1.4 Python 3.6 CPU Optimized)` kernel on SageMaker Studio, or `conda_pytorch_p36` on classic SageMaker Notebook Instances.\n",
    "\n",
    "---\n",
    "\n",
    "Following on from the previous 'local version' notebook, we show here how to trigger the model training and deployment on separate infrastructure - to make better use of resources.\n",
    "\n",
    "Note that you can safely ignore the WARNING about the pip version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install necessary libraries that may not be part of the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install some libraries which might not be available across all kernels (e.g. in Studio):\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.5  # Depending on your PyTorch version https://pypi.org/project/torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Execution Role and Session\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download News Aggregator Dataset\n",
    "\n",
    "We will download **FastAi AG News** dataset from the https://registry.opendata.aws/fast-ai-nlp/ public repository. This dataset contains a table of news headlines and their corresponding classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.preprocessing\n",
    "\n",
    "util.preprocessing.download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the dataset\n",
    "\n",
    "We will load the ag_news_csv/train.csv file to a Pandas dataframe for our data processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"CATEGORY\", \"TITLE\", \"CONTENT\"]\n",
    "# we use the train.csv only\n",
    "df = pd.read_csv(\"data/ag_news_csv/train.csv\", names=column_names, header=None, delimiter=\",\")\n",
    "# shuffle the DataFrame rows\n",
    "df = df.sample(frac = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll **only use**:\n",
    "\n",
    "- The **title** (Headline) of the news story, as our input\n",
    "- The **category**, as our target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has **four article categories:**\n",
    "\n",
    "- World (1)\n",
    "- Sports (2)\n",
    "- Business (3)\n",
    "- Sci/Tech (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Pre-Processing\n",
    "\n",
    "We'll do some basic processing of the text data to convert it into numerical form that the algorithm will be able to consume to create a model.\n",
    "\n",
    "We will do typical pre processing for NLP workloads such as: dummy encoding the labels, tokenizing the documents and set fixed sequence lengths for input feature dimension, padding documents to have fixed length input vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Encode the Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y, labels = util.preprocessing.dummy_encode_labels(df, \"CATEGORY\")\n",
    "print(labels)\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CATEGORY\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Set Fixed Sequence Lengths\n",
    "\n",
    "We want to describe our inputs at the more meaningful word level (rather than individual characters), and ensure a fixed length of the input feature dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs, tokenizer = util.preprocessing.tokenize_and_pad_docs(df, \"TITLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TITLE\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Word Embeddings\n",
    "\n",
    "To represent our words in numeric form, we'll use pre-trained vector representations for each word in the vocabulary: In this case we'll be using pre-built GloVe word embeddings.\n",
    "\n",
    "You could also explore training custom, domain-specific word embeddings using SageMaker's built-in [BlazingText algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html). See the official [blazingtext_word2vec_text8 sample](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/blazingtext_word2vec_text8) for an example notebook showing how.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = util.preprocessing.get_word_embeddings(tokenizer, \"data/embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "593bfd0a-b703-4e87-96dd-a7eb98e6940e",
    "_uuid": "f71c5f0b731d3418d3cb83be758233b5030da29d"
   },
   "outputs": [],
   "source": [
    "np.save(\n",
    "    file=\"./data/embeddings/docs-embedding-matrix\",\n",
    "    arr=embedding_matrix,\n",
    "    allow_pickle=False,\n",
    ")\n",
    "vocab_size=embedding_matrix.shape[0]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets\n",
    "\n",
    "Finally we need to divide our data into model training and evaluation sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    processed_docs,\n",
    "    encoded_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "np.save(\"./data/train/train_X.npy\", X_train)\n",
    "np.save(\"./data/train/train_Y.npy\", y_train)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "np.save(\"./data/test/test_X.npy\", X_test)\n",
    "np.save(\"./data/test/test_Y.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to S3\n",
    "\n",
    "We'll need to upload our processed data to S3 to make it available for SageMaker training jobs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sess.default_bucket()\n",
    "s3_prefix = \"news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = f\"{s3_prefix}/data/train\"\n",
    "testdata_s3_prefix = f\"{s3_prefix}/data/test\"\n",
    "embeddings_s3_prefix = f\"{s3_prefix}/data/embeddings\"\n",
    "output_s3 = f\"s3://{s3_bucket}/{s3_prefix}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path=\"./data/train/\", bucket=s3_bucket, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path=\"./data/test/\", bucket=s3_bucket, key_prefix=testdata_s3_prefix)\n",
    "embeddings_s3 = sess.upload_data(\n",
    "    # Only send the numpy array of embeddings, not the original files as well:\n",
    "    path=\"./data/embeddings/docs-embedding-matrix.npy\",\n",
    "    bucket=s3_bucket,\n",
    "    key_prefix=embeddings_s3_prefix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = { \"train\": train_s3, \"test\": test_s3, \"embeddings\": embeddings_s3 }\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Differentiated Infrastructure on Sagemaker\n",
    "\n",
    "This time, we've packaged the model build and train code in the [**main.py**](src/main.py) script in the **src** directory.\n",
    "\n",
    "We'll use the high-level PyTorch SDK to train our model on SageMaker.\n",
    "\n",
    "You can explore the script file for more details on the interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Amazon SageMaker runs your PyTorch script with pre-built containers\n",
    "\n",
    "Amazon Sagemaker has pre packaged a set of Docker images to help you accelerate building your projects. This what is driving the Sagemaker PyTorch Estimator. You can use the same PyTorch image for training and/or hosting. You can find more information in the following: https://github.com/aws/sagemaker-pytorch-training-toolkit , https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html, https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html\n",
    "\n",
    "\n",
    "#### Running your container during training\n",
    "\n",
    "When Amazon SageMaker runs training, your training script (entry_point input) is run just like a regular Python program. A number of files are laid out for your use, under a `/opt/ml` directory. These will be locations that you can access from within your script. You will see an example of the use of this in our [**main.py**](src/main.py) :\n",
    "\n",
    "    /opt/ml\n",
    "    |-- code\n",
    "    |   `-- <our script(s)>\n",
    "    |-- input\n",
    "    |   |-- config\n",
    "    |   |   |-- hyperparameters.json\n",
    "    |   |   `-- resourceConfig.json\n",
    "    |   `-- data\n",
    "    |       `-- <channel_name>\n",
    "    |           `-- <input data>\n",
    "    |-- model\n",
    "    |   `-- <model files>\n",
    "    `-- output\n",
    "        `-- failure\n",
    "\n",
    "##### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn't support distributed training, we'll ignore it here.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it's generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "##### The output\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it will be ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the script will run on a separate container, we can pass whatever parameters it needs through SageMaker:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = { \"epochs\": 5, \"vocab_size\": vocab_size, \"num_classes\": 4 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `TensorFlow` estimator object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `TensorFlow` estimator classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the training job using a ml.p3.2xlarge instance (GPUs) to accelerate our training. If your account runs into resource limits please use a ml.c5.xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorchEstimator(\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    role=role,\n",
    "    entry_point=\"main.py\",\n",
    "    source_dir=\"./src\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=[\n",
    "        { \"Name\": \"Epoch\", \"Regex\": \"epoch: ([0-9\\\\.]+)\" },\n",
    "        { \"Name\": \"Train:Loss\", \"Regex\": \"train_loss: ([0-9\\\\.]+)\" },\n",
    "        { \"Name\": \"Validation:Loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\" },\n",
    "        { \"Name\": \"Validation:Accuracy\", \"Regex\": \"val_acc: ([0-9\\\\.]+)\" },\n",
    "    ],\n",
    "    base_job_name=\"news-pytorch\",\n",
    "    max_run=20*60,  # Maximum allowed active runtime\n",
    "    use_spot_instances=True,  # Use spot instances to reduce cost\n",
    "    max_wait=30*60,  # Maximum clock time (including spot delays)\n",
    ")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model: Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (**JupyterLab Only**) Installing IPyWidgets Extension\n",
    "\n",
    "This notebook uses a fun little interactive widget to query the classifier, so **ONLY if you're using JupyterLab** (no action needed for plain Jupyter users) you'll need to install an extension to enable it:\n",
    "\n",
    "- Select \"*Settings > Enable Extension Manager (experimental)*\" from the toolbar, and confirm to enable it\n",
    "- Click on the new jigsaw puzzle piece icon in the sidebar, to open the Extension Manager\n",
    "- Search for `@jupyter-widgets/jupyterlab-manager` (Scroll down - search results show up *below* the list of currently installed widgets!)\n",
    "- Click \"**Install**\" below the widget's description\n",
    "- Wait for the blue progress bar that appears by the search box\n",
    "- You should be prompted \"*A build is needed to include the latest changes*\" - select \"**Rebuild**\"\n",
    "- The progress bar should resume, and you should shortly see a \"Build Complete\" dialogue.\n",
    "- Select \"**Reload**\" to reload the webpage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your model should now be in production as a RESTful API!\n",
    "\n",
    "Let's evaluate our model with some example headlines...\n",
    "\n",
    "If you struggle with the widget, you can always simply call the `classify()` function from Python.\n",
    "\n",
    "You can be creative with your headlines!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "def classify(text):\n",
    "    \"\"\"Classify a headline and print the results\"\"\"\n",
    "    processed = tokenizer.preprocess(text)\n",
    "    padded = tokenizer.pad([processed])\n",
    "    final_text = []\n",
    "    for w in padded[0]:\n",
    "        final_text.append(tokenizer.vocab.stoi[w])\n",
    "    result = predictor.predict([final_text])\n",
    "    print(result)\n",
    "    ix = np.argmax(result)\n",
    "    print(f\"Predicted class: '{labels[ix]}' with confidence {result[0][ix]:.2%}\")\n",
    "\n",
    "interaction = widgets.interact_manual(\n",
    "    classify,\n",
    "    text=widgets.Text(\n",
    "        value=\"The markets were bullish after news of the merger\",\n",
    "        placeholder=\"Type a news headline...\",\n",
    "        description=\"Headline:\",\n",
    "        layout=widgets.Layout(width=\"99%\"),\n",
    "    )\n",
    ")\n",
    "interaction.widget.children[1].description = \"Classify!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Unlike training jobs (which destroy their resources as soon as training is finished), real-time endpoint deployments provision instances until we specifically shut the endpoint down...\n",
    "\n",
    "So let's be frugal with resources, and delete resources when we don't need them anymore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Automatic Hyperparameter Optimization - HPO\n",
    "\n",
    "Rather than manually tweak parameters to tune the model performance, we can get SageMaker to help us out.\n",
    "\n",
    "We'll simply tell SageMaker:\n",
    "\n",
    "- The type and allowable range of each parameter,\n",
    "- The metric we want to optimize for, and\n",
    "- Strategy and resource constraints\n",
    "\n",
    "...and the service will set up jobs for us to find the best combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Hyper-)Parameter Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner, IntegerParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"epochs\": IntegerParameter(2, 7),\n",
    "    \"learning_rate\": ContinuousParameter(0.01, 0.2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Metric\n",
    "\n",
    "'Metrics' in SageMaker are scraped from the console output of jobs, by way of regular expressions.\n",
    "\n",
    "We can define multiple metrics to monitor, but HPO requires us to specify that exactly one of them is the **objective** metric to optimize:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{ \"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\" }]\n",
    "objective_metric_name = \"loss\"\n",
    "objective_type = \"Minimize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Tuning Job\n",
    "\n",
    "We already defined our Estimator above, so we'll just re-use the configuration with minor adjustments.\n",
    "\n",
    "Note that the Estimator's `hyperparameters` will be used as base values, and overridden by the HyperParameterTuner where appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep per-job resources modest, so that parallel jobs don't hit any limits:\n",
    "estimator.instance_type = \"ml.c5.xlarge\"\n",
    "estimator.instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    base_tuning_job_name=\"news-hpo-pytorch\",\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type\n",
    ")\n",
    "\n",
    "tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check On Progress\n",
    "\n",
    "HPO jobs can take a long time to complete, and can run multiple training jobs in parallel - each on multiple instances... Which is why the `fit()` call above doesn't wait by default, and won't show us a potentially-confusing consolidated log stream.\n",
    "\n",
    "Go to the Training > Hyperparameter Tuning Jobs page of the [**SageMaker Console**](https://console.aws.amazon.com/sagemaker/home#/hyper-tuning-jobs) and select the job from the list.\n",
    "\n",
    "You can see all the training jobs triggered for the HPO run, as well as overall summary metrics.\n",
    "\n",
    "This information can be accessed via the API/SDKs too of course. For example we can wait for HPO to finish like the below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Wait until HPO is finished\n",
    "hpo_state = None\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "while hpo_state is None or hpo_state == \"InProgress\":\n",
    "    if hpo_state is not None:\n",
    "        print(\"-\", end=\"\")\n",
    "        time.sleep(60)  # Poll once every 1 min\n",
    "    hpo_state = smclient.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    "    )[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "print(\"\\nHPO state:\", hpo_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "\n",
    "Just like with our `estimator`, we can call `tuner.deploy()` to create an endpoint and `predictor` from the best-performing model found in the HPO run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this notebook, we refactored our local code to train and deploy the same Keras model using SageMaker.\n",
    "\n",
    "The benefits of this approach are:\n",
    "\n",
    "- We can automatically provision specialist computing resources (e.g. high-performance, or GPU-accelerated instances) for **only** the duration of the training job: Getting good performance in training, without leaving resources sitting around under-utilized\n",
    "- Our trained model can be deployed to a secure, production-ready web endpoint with just one SDK call: No container or web application packaging required, unless we want to customize the behaviour\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.4 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/pytorch-1.4-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
